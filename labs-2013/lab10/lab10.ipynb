{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Lab: Classification and Clustering Grab Bag\n",
    "\n",
    "In this final lab, we will introduce two methods of classification called Neural Networks and Support Vector Machines, and an additional method of clustering called Hierarchical Agglomerative Clustering. We hope that introducing these methods will broaden your toolkit for tackling problems both for the final project and in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "import math\n",
    "import scipy\n",
    "import random\n",
    "import brewer2mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "A neural network \"learns\" by solving an optimization problem to choose a set of parameters that minimizes an error function, which is typically a squared error loss. This definition of learning isn't unique to a neural network model. Consider, in the simplest case, a linear model of the form $y = XB$. Given a vector of data  $y \\in \\mathbb{R}^m$, we choose  $B \\in \\mathbb{R}^n$ that minimizes $||y-XB||_2$, where $X$ is an $m$ by $n$ matrix with \"training data\" on the rows.  Solving this least squares minimization problem has a nice well known closed form analytical result: $\\hat{B} = (X^TX)^{-1}X^Ty$. There is, however, a tradeoff between computational ease of finding optimal model parameters and model complexity.  This is evident in the linear case since the model is extremely easy to fit but has a very simple form. The neural network attempts to find a \"sweet spot\": while the model is highly non-linear, its particular functional form allows for a computationally slick fitting procedure called \"backpropogation\". \n",
    "\n",
    "A \"neural network\" is a function from $f: \\mathbb{R}^m \\rightarrow \\{-1,1\\}$. The input to each neuron is a linear combination of the outputs of each of the neurons in the lower layer. The output of the neuron is a nonlinear threshold applied to its input: it's something that maps the real line to (-1,1) in a 1-1 fashion such that \"most\" of the positive line is mapped pretty close to 1 and and \"most\" of the negative line is mapped pretty close to -1. A common choice is the function  $g(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$.The final output of the signal is the sign of the top neuron. Note that the top layer is constrained to have a single neuron.\n",
    "\n",
    "The goal of fitting the model is to find a suitable set of  \"weights\", which we can compactly refer to as $w$, for the inputs of each of the neurons. One way to do this is to choose a set of weights that minimize the sum of squared errors for training examples we already have. The intuition here is to choose a model that is \"close\" to the true model, just like we would in a linear regression. However, unlike linear regression an analytical solution is not feasible because of the ugly threshold functions, so we need to resort to a computational approach like gradient descent. Gradient descent takes steps in the direction of greatest error decrease in the parameter space, hoping to find a (global) minimum. (Recall that from multivariable calculus, the gradient of a scalar field points in the direction of the greatest increase of the function, and so walking in the opposite direction points in the direction of greatest decrease repeating the argument with the negative of the function.) \n",
    "\n",
    "We start by initializing the model with some arbitrary set of weights. Feeding forward, given an input $x \\in \\mathbb{R}^m$, we may compute the output as delineated above. Hence, we may calculate the error $E = (y-f(x))^2$. Now, we compute the gradient of this error function with respect to the weights. \n",
    "\n",
    "The beauty of the method is that computing the gradient of the error is computationally slick and can be done recursively using the chain rule. To see this, we'll introduce some notation. Let $k = 1...L$ indicate layers, assume that $s_{jk}$ is the output of the jth neuron in layer $k$, $x_{jk}$ is the input into neuron $j$ in layer $k$, and $w_{ijk}$ is the weight for the signal input into the $j$th neuron in layer $k$ coming from the ith neuron  in layer $k-1$, so that $x_{jk} = \\sum_{i=1}^{d_{k-1}}w_{ijk}s_{ik-1}$, $s_{jk} = g(x_{jk})$. Note that $d_{k-1}$ stands for the number of neurons in layer $k-1$.  By the chain rule, $$\\frac{\\partial E}{\\partial w_{ijk}} = \\frac{\\partial E}{\\partial s_{jk}}\\frac{\\partial s_{jk}}{\\partial w_{ijk}}$$. Since $s_{jk}$ is linear in the weights, the tricky part is only in computing the former component of the product. We may recursively compute this as \n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial s_{jk}} = \\frac{\\partial E}{\\partial x_{jk}}\\frac{\\partial x_{jk}}{\\partial s_{jk}} = \\sum_{i=1}^{d_{k+1}} \\frac{\\partial E}{\\partial s_{ik+1}}\\frac{\\partial s_{ik+1}}{\\partial x_{jk}}\\frac{\\partial x_{jk}}{\\partial s_{jk}}$$\n",
    "\n",
    "The recursion terminates since $\\frac{\\partial E}{\\partial x_{1L}} = 2(x_{1L}-y)$. We start by computing the gradients from the top layer, and store the gradients as we progress down each layer,so that we need not recompute them. This leads to computational efficiency.  \n",
    "\n",
    "Thus \"learning\" in a neural network  is nothing but switching between computing the error using the training data and updating the weights by calculating the gradient of the error function. In the code block that follows, we write a class for NeuralNetworks, and apply it to performing classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Neural Network Class \n",
    "class Neural_Net:\n",
    "\n",
    "\t#constructor initializes a new neural network with randomly selected weights and pre-specified height, and number of neurons per layer\n",
    "\tdef __init__(self,non,height):\n",
    "\t\t#list to store the number of neurons in each layer of the network\n",
    "\t\tself.num_of_neurons = non\n",
    "\t\t#height of the network\n",
    "\t\tself.L = height\n",
    "\t\t#list to store number of weights in each layer of the network, indexed by layer, output neuron, input neuron\n",
    "\t\tself.weights = numpy.zeros(shape=(10,10,10))\n",
    "\t\t#delta_matrix: stores the gradient that is used in backpropagation\n",
    "\t\tself.deltas = numpy.zeros(shape=(10,10))\n",
    "\t\t#matrix that stores thresholded signals\n",
    "\t\tself.signals = numpy.zeros(shape=(10,10))\n",
    "\t\t#(tunable) learning_rate used in backpropagation\n",
    "\t\tself.learning_rate = .001\n",
    "\t\t#initialize weights to be between -2 and 2\n",
    "\t\tfor i in range(1,self.L+1):\n",
    "\t\t\tfor j in range(1,self.num_of_neurons[i]+1):\n",
    "\t\t\t\tfor k in range(self.num_of_neurons[i-1]+1):\n",
    "\t\t\t\t\tself.weights[i][j][k] = random.random()*4-2\t\t\n",
    "\t\t\t\t\t\t\t\t\t\t\t\n",
    "\t#forward_pass computes the output of the neural network given an input\n",
    "\tdef forward_pass(self,x):\n",
    "\t\t#(for convenience, we index neurons starting at 1 instead of zero)\n",
    "\t\tself.signals[0][0] = -1\n",
    "\t\tfor i in range(1,self.num_of_neurons[0]+1):\n",
    "\t\t\tself.signals[0][i] = x[i-1]\n",
    "\t\tfor i in range(1,self.L+1):\n",
    "\t\t\tself.signals[i][0] = -1\n",
    "\t\t\tfor j in range(1,self.num_of_neurons[i]+1):\n",
    "\t\t\t\tself.signals[i][j] = self.compute_signal(i,j)\n",
    "\t\treturn self.signals[self.L][1]\n",
    "\t\t\t\t\t\n",
    "\t#tune_weights performs the backpropagation algorithm given a training example as input\n",
    "\tdef tune_weights(self,y):\n",
    "\t\tself.deltas[self.L][1] = 2*(self.signals[self.L][1]-y)*(1-math.pow(self.signals[self.L][1],2))\n",
    "\t\tfor i in range(self.L-1,0,-1):\n",
    "\t\t\tfor j in range(1,self.num_of_neurons[i]+1):\n",
    "\t\t\t\tself.deltas[i][j] = self.compute_delta(i,j)\n",
    "\t\tfor i in range(1,self.L+1):\n",
    "\t\t\tfor j in range(1,self.num_of_neurons[i]+1):\n",
    "\t\t\t\tfor k in range(self.num_of_neurons[i-1]+1):\n",
    "\t\t\t\t\tself.weights[i][j][k] = self.weights[i][j][k]-self.learning_rate*self.signals[i-1][k]*self.deltas[i][j]\n",
    "\t\n",
    "\t#compute_signal: computes the delta for a given neuron at a given level\n",
    "\tdef compute_signal(self,level,neuron):\n",
    "\t\ts = 0\n",
    "\t\tfor i in range(self.num_of_neurons[level-1]+1):\n",
    "\t\t\ts += self.weights[level][neuron][i]*self.signals[level-1][i]\n",
    "\t\treturn self.g(s)\n",
    "\t\n",
    "\t#compute_delta: computes the signal s for a given neuron at a given level\n",
    "\tdef compute_delta(self,level,neuron):\n",
    "\t\ts = 0\n",
    "\t\tfor j in range(1,self.num_of_neurons[level+1]+1):\n",
    "\t\t\ts += self.weights[level+1][j][neuron]*self.deltas[level+1][j]\n",
    "\t\treturn (1-math.pow(self.signals[level][neuron],2))*s\n",
    "\t\n",
    "\t#soft threshold function\n",
    "\tdef g(self,s):\n",
    "\t\treturn (math.exp(s)-math.exp(-s))/(math.exp(s)+math.exp(-s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a neural network and see how well it performs on the test and training sets epoch by epoch. We will use a mock training and test set with two covariates. We instantiate a neural network with one hidden layer with four neurons, and a learning rate of .001. The learning rate is how much we scale the gradient in \"walking\" the parameter space. \n",
    "\n",
    "To gain some intuition, try to tweak some of these knobs.\n",
    "\n",
    "Note that this will take about a minute to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c246482a341e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m#compute the test errors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mtest_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_error\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtesting\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;31m#compute the training errors, SEQUENTIALLY. In other words, we perform backpropagation for *every* example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m#instead of all at once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-95702b2e4c8d>\u001b[0m in \u001b[0;36mforward_pass\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_of_neurons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "#read in the train and test dat, assuming csv format\n",
    "training = numpy.genfromtxt('train.csv',delimiter = ',')\n",
    "testing = numpy.genfromtxt('test.csv',delimiter = ',')\n",
    "\n",
    "#specify the number of neurons in each layer\n",
    "num_of_neurons = [2,4,1]\n",
    "\n",
    "#initialize a new neural network\n",
    "network = Neural_Net(num_of_neurons,2)\n",
    "\n",
    "#store the training error and test error during each epoch\n",
    "training_error = 0\n",
    "test_error = 0\n",
    "\n",
    "#store the training and test error for all epochs\n",
    "train = numpy.zeros(shape = (1000))\n",
    "test = numpy.zeros(shape = (1000))\n",
    "\n",
    "for epoch in range(1000):\n",
    "    training_error = 0\n",
    "    test_error = 0\n",
    "    #compute the test errors\n",
    "    for j in range(250):\n",
    "        test_error = test_error+math.pow(network.forward_pass(testing[j]) - testing[j][2], 2)\n",
    "    #compute the training errors, SEQUENTIALLY. In other words, we perform backpropagation for *every* example\n",
    "    #instead of all at once. \n",
    "    for i in range(25):\n",
    "        training_error = training_error+math.pow(network.forward_pass(training[i])- training[i][2], 2)\n",
    "        network.tune_weights(training[i][2])\t   \n",
    "    training_error = training_error/25\n",
    "    test_error = test_error/250\n",
    "    train[epoch] = training_error\n",
    "    test[epoch]  = test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the training and test error, epoch by epoch. Do we see signs of overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x76b3c50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEPCAYAAACUb2mtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkqTWGTaSpNb9f6qVye1HmLeKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x77ffc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(numpy.arange(1000), test, lw=2, label = 'test')\n",
    "ax.plot(numpy.arange(1000), train, lw=2, label = 'train')\n",
    "ax.legend(loc=0)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
